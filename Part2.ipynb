{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - The hidden layer\n",
    "\n",
    "Welcome to Part 2!\n",
    "\n",
    "Here, we'll implement a neural network with 2 layers (a hidden and output layer), and talk about feedforward and backpropagation. We'll take advantage of Numpy, a library that provides fast alternatives to math operations in Python and is designed to work efficiently with groups of numbers - like matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "\n",
    "- [Multi-layer perceptron](#multi-layer-perceptron)\n",
    "- [The weights](#the-weigts)\n",
    "- [Feedforward](#feedforward)\n",
    "  - The hidden layer\n",
    "  - The output layer\n",
    "- Backpropagation\n",
    "  - Caculating the error\n",
    "  - Learning\n",
    "- Full implementation in a real case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer perceptron <a id='multi-layer-perceptron'></a>\n",
    "\n",
    "In [Part 1](Part1.ipynb) we implemented the simplest neural network - a perceptron. This neural network doesn't have a hidden layer so it can't help us to find predictions to complex problems.\n",
    "\n",
    "When we combine perceptrons so that the output of one becomes the input of another one, we form a multi-layer perceptron or a neural network.\n",
    "\n",
    "Neural networks have a certain special architecture with layers:\n",
    "\n",
    "![Neural network](img/part2/1.png)\n",
    "\n",
    "- **Input layer:** contains the inputs $x_1$, $x_2$, $...$, $x_n$, $1$.\n",
    "- **Hidden layer:** set of linear models created with the first input layer.\n",
    "- **Output layer:** where the linear models get combined to obtain a nonlinear model.\n",
    "\n",
    "Now, not all neural networks look like the one above. They can be way more complicated. In particular, we can do the following things:\n",
    "- Add more nodes to the input, hidden and output layers.\n",
    "- Add more layers.\n",
    "\n",
    "The following image shows the network with which we will work, with its input units, labeled $x_1$, $x_2$, and $x_3$, its hidden nodes labeled $h_1$ and $h_2$, and all of the weights between the input layer and the hidden layer, labeled with their appropriate $w_{ij}$ indices:\n",
    "\n",
    "![Neural network for this exercise](img/part2/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The weigts <a id='the-weigts'></a>\n",
    "\n",
    "The weights need to be stored in a **matrix**, indexed as $w_{ij}$. Each **row** in the matrix will correspond to the weights **leading out** of a **single input unit**, and each **column** will correspond to the weights **leading in** to a **single hidden unit**.\n",
    "\n",
    "For our three input units and two hidden units, the weights matrix looks like this:\n",
    "\n",
    "![Weights matrix](img/part2/3.png)\n",
    "\n",
    "To initialize these weights in Numpy, we have to provide the shape of the matrix. If `features` is a 2D array containing the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(21)\n",
    "\n",
    "features = np.array([\n",
    "    [1, 2, 3], \n",
    "    [4, 5, 6]])\n",
    "\n",
    "# Number of records and input units\n",
    "n_records, n_inputs = features.shape\n",
    "\n",
    "# Input to hidden weights\n",
    "n_hidden = 2\n",
    "weights_input_to_hidden = np.random.normal(0, n_inputs**-0.5, size=(n_inputs, n_hidden))\n",
    "\n",
    "# Hidden to output weights\n",
    "weights_hidden_to_output = np.random.normal(0, n_inputs**-0.5, size=n_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a 2D array (i.e. matrix) named `weights_input_to_hidden` with dimensions `n_inputs` by `n_hidden`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03000157 -0.06419907]\n",
      " [ 0.60148166 -0.72557877]\n",
      " [ 0.43034978 -0.98787735]]\n"
     ]
    }
   ],
   "source": [
    "print(weights_input_to_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a 2D array named `weights_hidden_to_output` with dimensions `n_hidden`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.11885586 -0.1354298 ]\n"
     ]
    }
   ],
   "source": [
    "print(weights_hidden_to_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward <a id='feedforward'></a>\n",
    "\n",
    "Feedforward is the process neural networks use to turn the input into an output:\n",
    "\n",
    "![Feedforward](img/part2/4.png)\n",
    "\n",
    "On a multi-layer perceptron or neural network, to calculate a prediction $\\hat {y}\\,$ we start with the unit vector $x$ and then we apply the first matrix $W^{(1)}$ and a sigmoid activation function to get the values in the second layer. Then we apply the second matrix $W^{(2)}$ and another sigmoid function to get the values on the third layer, and so on and so forth, until we get our final prediction $\\hat y$. And this is the feedforward process that neural networks use to obtain the prediction from the input vector.\n",
    "\n",
    "Let's see step by step this process with our neural network with two layers (hidden and output layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The hidden layer\n",
    "\n",
    "The input to a hidden unit is the sum of all the inputs multiplied by the hidden unit's weights. So for each hidden layer unit, $h_j$, we need to calculate the following:\n",
    "$$\n",
    "h_j = \\sum_i w_{ij}x_i\n",
    "$$\n",
    "\n",
    "To do that, we need to use **matrix multiplication**.\n",
    "\n",
    "In this case, we're multiplying the inputs (a row vector here) by the weights. To do this, you take the dot (inner) product of the inputs with each column in the weights matrix. For example, to calculate the input to the first hidden unit, $j=1$, you'd take the dot product of the inputs with the first column of the weights matrix, like so:\n",
    "\n",
    "![Input to the first hidden unit](img/part2/5.png)\n",
    "\n",
    "$$\n",
    "h_1 = w_{11}x_1 + w_{21}x_2 + w_{31}x_3\n",
    "$$\n",
    "\n",
    "And for the second hidden layer input, you calculate the dot product of the inputs with the second column. And so on and so forth.\n",
    "\n",
    "In Numpy, you can do this for all the inputs and all the outputs at once using `np.dot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "[1 2 3]\n",
      "\n",
      "weights_input_to_hidden\n",
      "[[-0.03000157 -0.06419907]\n",
      " [ 0.60148166 -0.72557877]\n",
      " [ 0.43034978 -0.98787735]]\n",
      "\n",
      "hidden_inputs\n",
      "[ 2.46401109 -4.47898866]\n"
     ]
    }
   ],
   "source": [
    "input = features[0]\n",
    "\n",
    "# Calculate the inputs for the hidden layer\n",
    "hidden_inputs = np.dot(input, weights_input_to_hidden)\n",
    "\n",
    "print('input')\n",
    "print(input)\n",
    "print('\\nweights_input_to_hidden')\n",
    "print(weights_input_to_hidden)\n",
    "print('\\nhidden_inputs')\n",
    "print(hidden_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the inputs for the hidden layer, you calculate the outputs of that hidden layer passing the inputs through an activation function, which in this case we use the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.92158004,  0.01121762])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "hidden_outputs = sigmoid(hidden_inputs)\n",
    "hidden_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this process we have our neural network with values in each of its hidden units:\n",
    "\n",
    "![Neural network with values in each of its hidden units](img/part2/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The output layer\n",
    "\n",
    "Now that you have the outputs for the hidden layer, it's time to calculate the input for the output unit, and this process is the same as with the hidden layer, but instead of multiplying the inputs by the hidden unit's weights, we multiply the outputs for the hidden layer by the output unit's weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_outputs\n",
      "[ 0.92158004  0.01121762]\n",
      "\n",
      "weights_hidden_to_output\n",
      "[-0.11885586 -0.1354298 ]\n",
      "\n",
      "output_inputs\n",
      "-0.11105438401\n"
     ]
    }
   ],
   "source": [
    "output_inputs = np.dot(hidden_outputs, weights_hidden_to_output)\n",
    "\n",
    "print('hidden_outputs')\n",
    "print(hidden_outputs)\n",
    "print('\\nweights_hidden_to_output')\n",
    "print(weights_hidden_to_output)\n",
    "print('\\noutput_inputs')\n",
    "print(output_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we calculate the sigmoid of the result to have our prediction $\\hat y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47226490306194757"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = sigmoid(output_inputs)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Prediction](img/part2/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - The hidden layer\n",
    "\n",
    "Welcome to Part 2!\n",
    "\n",
    "Here, we'll implement a neural network with 2 layers (a hidden and output layer), and talk about feedforward and backpropagation. We'll take advantage of Numpy, a library that provides fast alternatives to math operations in Python and is designed to work efficiently with groups of numbers - like matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "\n",
    "- [Multi-layer perceptron](#multi-layer-perceptron)\n",
    "- [The weights](#the-weigts)\n",
    "- [Feedforward](#feedforward)\n",
    "  - The hidden layer\n",
    "  - The output layer\n",
    "- Backpropagation\n",
    "  - Caculating the error\n",
    "  - Learning\n",
    "- Full implementation in a real case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer perceptron <a id='multi-layer-perceptron'></a>\n",
    "\n",
    "In [Part 1](Part1.ipynb) we implemented the simplest neural network - a perceptron. This neural network doesn't have a hidden layer so it can't help us to find predictions to complex problems.\n",
    "\n",
    "When we combine perceptrons so that the output of one becomes the input of another one, we form a multi-layer perceptron or a neural network.\n",
    "\n",
    "Neural networks have a certain special architecture with layers:\n",
    "\n",
    "![Neural network](img/part2/1.png)\n",
    "\n",
    "- **Input layer:** contains the inputs $x_1$, $x_2$, $...$, $x_n$, $1$.\n",
    "- **Hidden layer:** set of linear models created with the first input layer.\n",
    "- **Output layer:** where the linear models get combined to obtain a nonlinear model.\n",
    "\n",
    "Now, not all neural networks look like the one above. They can be way more complicated. In particular, we can do the following things:\n",
    "- Add more nodes to the input, hidden and output layers.\n",
    "- Add more layers.\n",
    "\n",
    "The following image shows the network with which we will work, with its input units, labeled $x_1$, $x_2$, and $x_3$, its hidden nodes labeled $h_1$ and $h_2$, and all of the weights between the input layer and the hidden layer, labeled with their appropriate $w_{ij}$ indices:\n",
    "\n",
    "![Neural network for this exercise](img/part2/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The weigts <a id='the-weigts'></a>\n",
    "\n",
    "The weights need to be stored in a **matrix**, indexed as $w_{ij}$. Each **row** in the matrix will correspond to the weights **leading out** of a **single input unit**, and each **column** will correspond to the weights **leading in** to a **single hidden unit**.\n",
    "\n",
    "For our three input units and two hidden units, the weights matrix looks like this:\n",
    "\n",
    "![Weights matrix](img/part2/3.png)\n",
    "\n",
    "To initialize these weights in Numpy, we have to provide the shape of the matrix. If `features` is a 2D array containing the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(21)\n",
    "\n",
    "features = np.array([\n",
    "    [1, 2, 3], \n",
    "    [4, 5, 6]])\n",
    "\n",
    "# Number of records and input units\n",
    "n_records, n_inputs = features.shape\n",
    "\n",
    "# Input to hidden weights\n",
    "n_hidden = 2\n",
    "weights_input_to_hidden = np.random.normal(0, n_inputs**-0.5, size=(n_inputs, n_hidden))\n",
    "\n",
    "# Hidden to output weights\n",
    "weights_hidden_to_output = np.random.normal(0, n_inputs**-0.5, size=n_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a 2D array (i.e. matrix) named `weights_input_to_hidden` with dimensions `n_inputs` by `n_hidden`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03000157 -0.06419907]\n",
      " [ 0.60148166 -0.72557877]\n",
      " [ 0.43034978 -0.98787735]]\n"
     ]
    }
   ],
   "source": [
    "print(weights_input_to_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a 2D array named `weights_hidden_to_output` with dimensions `n_hidden`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.11885586 -0.1354298 ]\n"
     ]
    }
   ],
   "source": [
    "print(weights_hidden_to_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward <a id='feedforward'></a>\n",
    "\n",
    "Feedforward is the process neural networks use to turn the input into an output:\n",
    "\n",
    "![Feedforward](img/part2/4.png)\n",
    "\n",
    "On a multi-layer perceptron or neural network, to calculate a prediction $\\hat {y}\\,$ we start with the unit vector $x$ and then we apply the first matrix $W^{(1)}$ and a sigmoid activation function to get the values in the second layer. Then we apply the second matrix $W^{(2)}$ and another sigmoid function to get the values on the third layer, and so on and so forth, until we get our final prediction $\\hat y$. And this is the feedforward process that neural networks use to obtain the prediction from the input vector.\n",
    "\n",
    "Let's see step by step this process with our neural network with two layers (hidden and output layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The hidden layer\n",
    "\n",
    "The input to a hidden unit is the sum of all the inputs multiplied by the hidden unit's weights. So for each hidden layer unit, $h_j$, we need to calculate the following:\n",
    "$$\n",
    "h_j = \\sum_i w_{ij}x_i\n",
    "$$\n",
    "\n",
    "To do that, we need to use **matrix multiplication**.\n",
    "\n",
    "In this case, we're multiplying the inputs (a row vector here) by the weights. To do this, you take the dot (inner) product of the inputs with each column in the weights matrix. For example, to calculate the input to the first hidden unit, $j=1$, you'd take the dot product of the inputs with the first column of the weights matrix, like so:\n",
    "\n",
    "![Input to the first hidden unit](img/part2/5.png)\n",
    "\n",
    "$$\n",
    "h_1 = w_{11}x_1 + w_{21}x_2 + w_{31}x_3\n",
    "$$\n",
    "\n",
    "And for the second hidden layer input, you calculate the dot product of the inputs with the second column. And so on and so forth.\n",
    "\n",
    "In Numpy, you can do this for all the inputs and all the outputs at once using `np.dot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "[1 2 3]\n",
      "\n",
      "weights_input_to_hidden\n",
      "[[-0.03000157 -0.06419907]\n",
      " [ 0.60148166 -0.72557877]\n",
      " [ 0.43034978 -0.98787735]]\n",
      "\n",
      "hidden_inputs\n",
      "[ 2.46401109 -4.47898866]\n"
     ]
    }
   ],
   "source": [
    "input = features[0]\n",
    "\n",
    "# Calculate the inputs for the hidden layer\n",
    "hidden_inputs = np.dot(input, weights_input_to_hidden)\n",
    "\n",
    "print('input')\n",
    "print(input)\n",
    "print('\\nweights_input_to_hidden')\n",
    "print(weights_input_to_hidden)\n",
    "print('\\nhidden_inputs')\n",
    "print(hidden_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the inputs for the hidden layer, you calculate the outputs of that hidden layer passing the inputs through an activation function, which in this case we use the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.92158004,  0.01121762])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "hidden_outputs = sigmoid(hidden_inputs)\n",
    "hidden_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this process we have our neural network with values in each of its hidden units:\n",
    "\n",
    "![Neural network with values in each of its hidden units](img/part2/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The output layer\n",
    "\n",
    "Now that you have the outputs for the hidden layer, it's time to calculate the input for the output unit, and this process is the same as with the hidden layer, but instead of multiplying the inputs by the hidden unit's weights, we multiply the outputs for the hidden layer by the output unit's weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_outputs\n",
      "[ 0.92158004  0.01121762]\n",
      "\n",
      "weights_hidden_to_output\n",
      "[-0.11885586 -0.1354298 ]\n",
      "\n",
      "output_inputs\n",
      "-0.11105438401\n"
     ]
    }
   ],
   "source": [
    "output_inputs = np.dot(hidden_outputs, weights_hidden_to_output)\n",
    "\n",
    "print('hidden_outputs')\n",
    "print(hidden_outputs)\n",
    "print('\\nweights_hidden_to_output')\n",
    "print(weights_hidden_to_output)\n",
    "print('\\noutput_inputs')\n",
    "print(output_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we calculate the sigmoid of the result to have our prediction $\\hat y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47226490306194757"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = sigmoid(output_inputs)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Prediction](img/part2/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Training is the process that looks for the parameters a neural network should have on its edges (weights) in order to model our data well. One method used for training a neural network is backpropagation.\n",
    "\n",
    "In a nutshell, backpropagation consists of:\n",
    "- Calculating the error of the prediction.\n",
    "- Running the feedforward operation backward (backpropagation) to spread the error to each of the weights.\n",
    "- Use this to update the weights, and get a better model (learn).\n",
    "- Continue this until we have a model that is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the error\n",
    "\n",
    "To update the weights to hidden layers we use an algorithm called gradient descent. In order to do this, you need to know how much error each of the hidden units contributed to the final output. Since the output of a layer is determined by the weights between layers, the error resulting from units is scaled by the weights going forward through the network. Since we know the error at the output, we can use the weights to work backward to hidden layers.\n",
    "\n",
    "You can view this process as flipping the network over and using the error as the input:\n",
    "\n",
    "![Backpropagation](img/part2/8.png)\n",
    "\n",
    "In the output layer, you have error terms $\\delta_k^0$ attributed to each output unit $k$:\n",
    "\n",
    "$$\n",
    "\\delta_k^0 = (y_k - \\hat {y_k}) \\, f'(output_k)\n",
    "$$\n",
    "\n",
    "Remember that we are using the sigmoid for the activation function $f(h) = 1/(1+e^{-h})$ and:\n",
    "\n",
    "$$\n",
    "f'(h) = f(h)(1 - f(h))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_error_term\n",
      "0.0318355158503\n"
     ]
    }
   ],
   "source": [
    "# The correct value we're trying to predict.\n",
    "target = 0.6\n",
    "\n",
    "# Calculate the network's output error\n",
    "error = target - output\n",
    "\n",
    "# Calculate the output layer's error term\n",
    "output_error_term = error * output * (1 - output)\n",
    "\n",
    "print('output_error_term')\n",
    "print(output_error_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the error attributed to hidden unit $j$ is the output error, scaled by the weights between the output and hidden layers (and the gradient):\n",
    "\n",
    "$$\n",
    "\\delta_j^h = \\sum W_{jk} \\, \\delta_k^0 \\, f'(h_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights_hidden_to_output\n",
      "[-0.11885586 -0.1354298 ]\n",
      "\n",
      "output_error_term\n",
      "0.0318355158503\n",
      "\n",
      "hidden_error\n",
      "[-0.00378384 -0.00431148]\n",
      "\n",
      "hidden_outputs\n",
      "[ 0.92158004  0.01121762]\n",
      "\n",
      "hidden_error_term\n",
      "[ -2.73458970e-04  -4.78219744e-05]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the hidden layer's error term\n",
    "hidden_error = output_error_term * weights_hidden_to_output\n",
    "hidden_error_term =  hidden_error * hidden_outputs * (1 - hidden_outputs)\n",
    "\n",
    "print('weights_hidden_to_output')\n",
    "print(weights_hidden_to_output)\n",
    "print('\\noutput_error_term')\n",
    "print(output_error_term)\n",
    "print('\\nhidden_error')\n",
    "print(hidden_error)\n",
    "print('\\nhidden_outputs')\n",
    "print(hidden_outputs)\n",
    "print('\\nhidden_error_term')\n",
    "print(hidden_error_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural network with error terms](img/part2/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "\"Learning\" takes our errors and tells each weight how it can change to reduce it.\n",
    "\n",
    "Then, the change to the weights will be:\n",
    "\n",
    "$$\n",
    "\\Delta w_{ij} = \\eta \\delta_j^h x_i\n",
    "$$\n",
    "\n",
    "where $w_{ij}$ are the weights between the inputs and hidden layer and $x_i$ are the input unit values. This form holds for however many layers there are. The weight steps are equal to the learning rate times the output error of the layer times the values of the inputs to that layer:\n",
    "\n",
    "$$\n",
    "\\Delta w_{pq} = \\eta \\delta_{output} V_{in}\n",
    "$$\n",
    "\n",
    "Here, you get the output error, $\\delta_{output}$, by propagating the errors backward from higher layers. And the input values, $V_{in}$ are the inputs to the layer, the hidden layer activations to the output unit for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_error_term = 0.0318355158503\n",
      "hidden_outputs =  [ 0.92158004  0.01121762]\n",
      "Change in weights for hidden layer to output layer:\n",
      "[  2.93389758e-03   3.57118668e-05]\n",
      "\n",
      "hidden_error_term =  [ -2.73458970e-04  -4.78219744e-05]\n",
      "input =  [1 2 3]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[[ -2.73458970e-05  -4.78219744e-06]\n",
      " [ -5.46917940e-05  -9.56439489e-06]\n",
      " [ -8.20376911e-05  -1.43465923e-05]]\n"
     ]
    }
   ],
   "source": [
    "learnrate = 0.1\n",
    "\n",
    "# Calculate change in weights for hidden layer to output layer\n",
    "delta_weights_hidden_to_output = learnrate * output_error_term * hidden_outputs\n",
    "\n",
    "# Calculate change in weights for input layer to hidden layer\n",
    "delta_weights_input_to_hidden = learnrate * hidden_error_term * input[:, None]\n",
    "\n",
    "print('output_error_term =', output_error_term)\n",
    "print('hidden_outputs = ', hidden_outputs)\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_weights_hidden_to_output)\n",
    "\n",
    "print('\\nhidden_error_term = ', hidden_error_term)\n",
    "print('input = ', input)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_weights_input_to_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the changes in weights computed, our new weights are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old weights for hidden layer to output layer:\n",
      "[-0.11885586 -0.1354298 ]\n",
      "\n",
      "Old weights for input layer to hidden layer:\n",
      "[[-0.03000157 -0.06419907]\n",
      " [ 0.60148166 -0.72557877]\n",
      " [ 0.43034978 -0.98787735]]\n",
      "\n",
      "\n",
      "New weights for hidden layer to output layer:\n",
      "[-0.11592196 -0.13539409]\n",
      "\n",
      "New weights for input layer to hidden layer:\n",
      "[[-0.03002892 -0.06420385]\n",
      " [ 0.60142697 -0.72558833]\n",
      " [ 0.43026774 -0.9878917 ]]\n"
     ]
    }
   ],
   "source": [
    "print('Old weights for hidden layer to output layer:')\n",
    "print(weights_hidden_to_output)\n",
    "\n",
    "print('\\nOld weights for input layer to hidden layer:')\n",
    "print(weights_input_to_hidden)\n",
    "\n",
    "# Calculate new weights\n",
    "new_weights_hidden_to_output = weights_hidden_to_output + delta_weights_hidden_to_output\n",
    "new_weights_input_to_hidden = weights_input_to_hidden + delta_weights_input_to_hidden\n",
    "\n",
    "print('\\n\\nNew weights for hidden layer to output layer:')\n",
    "print(new_weights_hidden_to_output)\n",
    "\n",
    "print('\\nNew weights for input layer to hidden layer:')\n",
    "print(new_weights_input_to_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may think that this was a tiny change in our weights, but this process was realized with the input data of just one record. In order to find a better model (with better weights and predictions), you have to iterate this process in your whole dataset a lot of times. We'll do this next.\n",
    "\n",
    "The takeaway is that you understand how all the pieces that allow a neural network \"learn\" fit together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
